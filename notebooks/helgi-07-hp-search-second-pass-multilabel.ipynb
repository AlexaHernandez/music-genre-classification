{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics genre multi label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook:\n",
    "* Uses the dataset generated by **helgi-05-scraped-lyrics-v2-preprocessing.ipynb**\n",
    "* Performs **multi** label classification on the genres available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Repos\\comp550-final-project\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random_state = 1111\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "import sklearn.multiclass\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.utils\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from libs.processing import (\n",
    "    Lemmatizer,\n",
    "    Stemmer,\n",
    "    StopWords,\n",
    "    fit_vectorizer,\n",
    "    random_search,\n",
    "    choose_random_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genres</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 Stones</td>\n",
       "      <td>3 Leaf Loser</td>\n",
       "      <td>Biting the hand that feeds you, lying to the v...</td>\n",
       "      <td>['Hard Rock', 'Rock']</td>\n",
       "      <td>Hard Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 Stones</td>\n",
       "      <td>Adrenaline</td>\n",
       "      <td>My heart is beating faster can't control these...</td>\n",
       "      <td>['Hard Rock', 'Rock']</td>\n",
       "      <td>Hard Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 Stones</td>\n",
       "      <td>Anthem For The Underdog</td>\n",
       "      <td>You say you know just who I am\\nBut you can't ...</td>\n",
       "      <td>['Hard Rock', 'Rock']</td>\n",
       "      <td>Hard Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 Stones</td>\n",
       "      <td>Anthem For The Underdog (Picture Perfect Sessi...</td>\n",
       "      <td>You say you know just who I am\\nBut you can't ...</td>\n",
       "      <td>['Hard Rock', 'Rock']</td>\n",
       "      <td>Hard Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Stones</td>\n",
       "      <td>Arms Of A Stranger</td>\n",
       "      <td>I came home early to see you,\\nCouldn't wait t...</td>\n",
       "      <td>['Hard Rock', 'Rock']</td>\n",
       "      <td>Hard Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58714</th>\n",
       "      <td>Ängie</td>\n",
       "      <td>Spun</td>\n",
       "      <td>Avoid eye contact, you want it, I know it's a ...</td>\n",
       "      <td>['Indie']</td>\n",
       "      <td>Indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58715</th>\n",
       "      <td>Ängie</td>\n",
       "      <td>Talk to Me Nice</td>\n",
       "      <td>Imma fuck you up\\nImma fuck you up\\nImma fuck ...</td>\n",
       "      <td>['Indie']</td>\n",
       "      <td>Indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58716</th>\n",
       "      <td>Ängie</td>\n",
       "      <td>Two Together</td>\n",
       "      <td>I don't even think that you're my type\\nYou're...</td>\n",
       "      <td>['Indie']</td>\n",
       "      <td>Indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58717</th>\n",
       "      <td>Ängie</td>\n",
       "      <td>Venus In Furs</td>\n",
       "      <td>Shiny, shiny, shiny boots of leather\\nWhiplash...</td>\n",
       "      <td>['Indie']</td>\n",
       "      <td>Indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58718</th>\n",
       "      <td>Ängie</td>\n",
       "      <td>We Run</td>\n",
       "      <td>Loving you is the hardest thing\\nI've ever don...</td>\n",
       "      <td>['Indie']</td>\n",
       "      <td>Indie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58719 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist                                               song  \\\n",
       "0      12 Stones                                       3 Leaf Loser   \n",
       "1      12 Stones                                         Adrenaline   \n",
       "2      12 Stones                            Anthem For The Underdog   \n",
       "3      12 Stones  Anthem For The Underdog (Picture Perfect Sessi...   \n",
       "4      12 Stones                                 Arms Of A Stranger   \n",
       "...          ...                                                ...   \n",
       "58714      Ängie                                               Spun   \n",
       "58715      Ängie                                    Talk to Me Nice   \n",
       "58716      Ängie                                       Two Together   \n",
       "58717      Ängie                                      Venus In Furs   \n",
       "58718      Ängie                                             We Run   \n",
       "\n",
       "                                                  lyrics  \\\n",
       "0      Biting the hand that feeds you, lying to the v...   \n",
       "1      My heart is beating faster can't control these...   \n",
       "2      You say you know just who I am\\nBut you can't ...   \n",
       "3      You say you know just who I am\\nBut you can't ...   \n",
       "4      I came home early to see you,\\nCouldn't wait t...   \n",
       "...                                                  ...   \n",
       "58714  Avoid eye contact, you want it, I know it's a ...   \n",
       "58715  Imma fuck you up\\nImma fuck you up\\nImma fuck ...   \n",
       "58716  I don't even think that you're my type\\nYou're...   \n",
       "58717  Shiny, shiny, shiny boots of leather\\nWhiplash...   \n",
       "58718  Loving you is the hardest thing\\nI've ever don...   \n",
       "\n",
       "                      genres   category  \n",
       "0      ['Hard Rock', 'Rock']  Hard Rock  \n",
       "1      ['Hard Rock', 'Rock']  Hard Rock  \n",
       "2      ['Hard Rock', 'Rock']  Hard Rock  \n",
       "3      ['Hard Rock', 'Rock']  Hard Rock  \n",
       "4      ['Hard Rock', 'Rock']  Hard Rock  \n",
       "...                      ...        ...  \n",
       "58714              ['Indie']      Indie  \n",
       "58715              ['Indie']      Indie  \n",
       "58716              ['Indie']      Indie  \n",
       "58717              ['Indie']      Indie  \n",
       "58718              ['Indie']      Indie  \n",
       "\n",
       "[58719 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('E:\\\\Repos\\\\comp550-final-project\\\\data\\\\scraped-lyrics-v2-preprocessed.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres are a string, let's convert to a list\n",
    "df.genres = df.genres.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of data used: 1*58719 = 58719\n"
     ]
    }
   ],
   "source": [
    "data_ratio = 1 # How much of the data to use\n",
    "df.index.to_numpy()\n",
    "splits = [0.7, 0.85]\n",
    "\n",
    "indices = sklearn.utils.shuffle(df.index.to_numpy(), random_state=1234)\n",
    "n_data_used = int(data_ratio*len(indices))\n",
    "\n",
    "all_data = []\n",
    "for i in indices[ : n_data_used]:\n",
    "    song = df.iloc[i]\n",
    "    all_data.append((song.lyrics, song.genres))\n",
    "\n",
    "all_data = np.array(all_data, dtype=object)\n",
    "n = len(all_data)\n",
    "data = {\n",
    "    'X_train_raw': all_data[:int(splits[0]*n), 0],\n",
    "    'y_train': all_data[:int(splits[0]*n), 1],\n",
    "    'X_valid_raw': all_data[int(splits[0]*n):int(splits[1]*n), 0],\n",
    "    'y_valid': all_data[int(splits[0]*n): int(splits[1]*n), 1],\n",
    "    'X_test_raw': all_data[int(splits[1]*n):, 0],\n",
    "    'y_test': all_data[int(splits[1]*n):, 1]\n",
    "}\n",
    "\n",
    "print(f'Ratio of data used: {data_ratio}*{len(indices)} = {n_data_used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set hyper-parameter search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    'data': {\n",
    "        'tokenizer': [Lemmatizer, Stemmer],\n",
    "        'stop_words': [StopWords('nltk_english'), StopWords('None', [])],\n",
    "        'min_df': [0, 1, 2, 3] # Minimum token frequency\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'eta0': [1e-3, 1e-2, 1e-1], # learning rate\n",
    "            'alpha': [1e-3, 1e-2, 1e-1], # regularization\n",
    "            'max_iter': np.arange(start=1, stop=10), # epochs\n",
    "            'loss': ['log'],\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(start=10, stop=1000, step=10),\n",
    "            'max_depth': np.arange(start=1, stop=10, step=2),\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': sklearn.linear_model.SGDClassifier,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB,\n",
    "    'random_forest': sklearn.ensemble.RandomForestClassifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set variation 1/4\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n",
      "Data set variation 2/4\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n",
      "Data set variation 3/4\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n",
      "Data set variation 4/4\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/4\n",
      "\tlogistic_regression 2/4\n",
      "\tlogistic_regression 3/4\n",
      "\tlogistic_regression 4/4\n",
      "\tnaive_bayes 1/4\n",
      "\tnaive_bayes 2/4\n",
      "\tnaive_bayes 3/4\n",
      "\tnaive_bayes 4/4\n",
      "\trandom_forest 1/4\n",
      "\trandom_forest 2/4\n",
      "\trandom_forest 3/4\n",
      "\trandom_forest 4/4\n"
     ]
    }
   ],
   "source": [
    "data_sets, results = random_search(models, data, search_params, n_datasets=4, n_models=4, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset variation 1: {'tokenizer': \"<class 'libs.processing.Lemmatizer'>\", 'stop_words': 'None', 'min_df': '1'}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 3, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.5353088101725704, 'TOP@2': 0.7100363306085377, 'TOP@3': 0.8005222524977293}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 4, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4510672116257947, 'TOP@2': 0.6068346957311535, 'TOP@3': 0.7006130790190735}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 5, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.37454586739327883, 'TOP@2': 0.5375794732061762, 'TOP@3': 0.6424841053587648}}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.001, 'max_iter': 8, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.473433242506812, 'TOP@2': 0.6380563124432335, 'TOP@3': 0.7338782924613987}}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.30000000000000004}, 'valid_accuracy': {'TOP@1': 0.5332652134423251, 'TOP@2': 0.7020890099909174, 'TOP@3': 0.7870118074477748}}\n",
      "\t\t{'model_params': {'alpha': 0.1}, 'valid_accuracy': {'TOP@1': 0.5220254314259763, 'TOP@2': 0.6905086285195277, 'TOP@3': 0.7782697547683923}}\n",
      "\t\t{'model_params': {'alpha': 0.5}, 'valid_accuracy': {'TOP@1': 0.5416666666666666, 'TOP@2': 0.7126475930971844, 'TOP@3': 0.7922343324250681}}\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': {'TOP@1': 0.5479109900090826, 'TOP@2': 0.7229791099000908, 'TOP@3': 0.7997275204359673}}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 3, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.41076294277929154, 'TOP@2': 0.6311307901907357, 'TOP@3': 0.7091280653950953}}\n",
      "\t\t{'model_params': {'n_estimators': 460, 'max_depth': 5, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.44436875567665757, 'TOP@2': 0.6608764759309719, 'TOP@3': 0.735127157129882}}\n",
      "\t\t{'model_params': {'n_estimators': 970, 'max_depth': 7, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.45674386920980925, 'TOP@2': 0.6796094459582198, 'TOP@3': 0.7580608537693007}}\n",
      "\t\t{'model_params': {'n_estimators': 320, 'max_depth': 9, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.46435059037238874, 'TOP@2': 0.6962988192552225, 'TOP@3': 0.7698683015440508}}\n",
      "\n",
      "Dataset variation 2: {'tokenizer': \"<class 'libs.processing.Stemmer'>\", 'stop_words': 'nltk_english', 'min_df': '2'}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 3, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.5364441416893733, 'TOP@2': 0.709014532243415, 'TOP@3': 0.7896230699364214}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 4, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.45810626702997276, 'TOP@2': 0.6271571298819255, 'TOP@3': 0.7235467756584922}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 5, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4271117166212534, 'TOP@2': 0.5895776566757494, 'TOP@3': 0.7029972752043597}}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.001, 'max_iter': 8, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.48455949137148047, 'TOP@2': 0.6485013623978202, 'TOP@3': 0.741371480472298}}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.30000000000000004}, 'valid_accuracy': {'TOP@1': 0.5255449591280654, 'TOP@2': 0.6917574931880109, 'TOP@3': 0.7807674841053588}}\n",
      "\t\t{'model_params': {'alpha': 0.1}, 'valid_accuracy': {'TOP@1': 0.5208900999091735, 'TOP@2': 0.6889191643960036, 'TOP@3': 0.7809945504087193}}\n",
      "\t\t{'model_params': {'alpha': 0.5}, 'valid_accuracy': {'TOP@1': 0.5290644868301544, 'TOP@2': 0.6986830154405086, 'TOP@3': 0.7842870118074478}}\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': {'TOP@1': 0.5391689373297003, 'TOP@2': 0.7025431425976385, 'TOP@3': 0.7914396003633061}}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 3, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4283605812897366, 'TOP@2': 0.6443006357856494, 'TOP@3': 0.7113987284287012}}\n",
      "\t\t{'model_params': {'n_estimators': 460, 'max_depth': 5, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4515213442325159, 'TOP@2': 0.668369663941871, 'TOP@3': 0.7446639418710264}}\n",
      "\t\t{'model_params': {'n_estimators': 970, 'max_depth': 7, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4634423251589464, 'TOP@2': 0.6886920980926431, 'TOP@3': 0.764872842870118}}\n",
      "\t\t{'model_params': {'n_estimators': 320, 'max_depth': 9, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4696866485013624, 'TOP@2': 0.6985694822888283, 'TOP@3': 0.7799727520435967}}\n",
      "\n",
      "Dataset variation 3: {'tokenizer': \"<class 'libs.processing.Lemmatizer'>\", 'stop_words': 'nltk_english', 'min_df': '2'}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 3, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.5303133514986376, 'TOP@2': 0.7073115349682108, 'TOP@3': 0.7887148047229791}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 4, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.45572207084468663, 'TOP@2': 0.6158038147138964, 'TOP@3': 0.7191189827429609}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 5, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.42370572207084467, 'TOP@2': 0.5940054495912807, 'TOP@3': 0.7041326067211626}}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.001, 'max_iter': 8, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.49216621253405995, 'TOP@2': 0.6527020890099909, 'TOP@3': 0.7520435967302452}}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.30000000000000004}, 'valid_accuracy': {'TOP@1': 0.52736148955495, 'TOP@2': 0.6934604904632152, 'TOP@3': 0.7838328792007266}}\n",
      "\t\t{'model_params': {'alpha': 0.1}, 'valid_accuracy': {'TOP@1': 0.5242960944595823, 'TOP@2': 0.6909627611262489, 'TOP@3': 0.7790644868301544}}\n",
      "\t\t{'model_params': {'alpha': 0.5}, 'valid_accuracy': {'TOP@1': 0.5331516802906449, 'TOP@2': 0.698455949137148, 'TOP@3': 0.7865576748410535}}\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': {'TOP@1': 0.5428019981834695, 'TOP@2': 0.707425068119891, 'TOP@3': 0.7916666666666666}}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 3, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.423819255222525, 'TOP@2': 0.639872842870118, 'TOP@3': 0.7087874659400545}}\n",
      "\t\t{'model_params': {'n_estimators': 460, 'max_depth': 5, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.44947774750227065, 'TOP@2': 0.6676884650317892, 'TOP@3': 0.7370572207084468}}\n",
      "\t\t{'model_params': {'n_estimators': 970, 'max_depth': 7, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4612851952770209, 'TOP@2': 0.6858537693006358, 'TOP@3': 0.7603315168029064}}\n",
      "\t\t{'model_params': {'n_estimators': 320, 'max_depth': 9, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.46900544959128065, 'TOP@2': 0.698455949137148, 'TOP@3': 0.7728201634877384}}\n",
      "\n",
      "Dataset variation 4: {'tokenizer': \"<class 'libs.processing.Lemmatizer'>\", 'stop_words': 'None', 'min_df': '2'}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 3, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.5354223433242506, 'TOP@2': 0.7096957311534968, 'TOP@3': 0.8004087193460491}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 4, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.46787011807447776, 'TOP@2': 0.6184150772025432, 'TOP@3': 0.7163941871026339}}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 5, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.3916893732970027, 'TOP@2': 0.5420072661217076, 'TOP@3': 0.6420299727520435}}\n",
      "\t\t{'model_params': {'eta0': 0.01, 'alpha': 0.001, 'max_iter': 8, 'loss': 'log', 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.48274296094459584, 'TOP@2': 0.6576975476839237, 'TOP@3': 0.7540871934604905}}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.30000000000000004}, 'valid_accuracy': {'TOP@1': 0.5113533151680291, 'TOP@2': 0.6841507720254314, 'TOP@3': 0.771457765667575}}\n",
      "\t\t{'model_params': {'alpha': 0.1}, 'valid_accuracy': {'TOP@1': 0.5088555858310627, 'TOP@2': 0.6826748410535877, 'TOP@3': 0.7706630336058129}}\n",
      "\t\t{'model_params': {'alpha': 0.5}, 'valid_accuracy': {'TOP@1': 0.5165758401453224, 'TOP@2': 0.6886920980926431, 'TOP@3': 0.7754314259763851}}\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': {'TOP@1': 0.5250908265213442, 'TOP@2': 0.6955040871934605, 'TOP@3': 0.7804268846503178}}\n",
      "\trandom_forest:\n",
      "\t\t{'model_params': {'n_estimators': 700, 'max_depth': 3, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4297229791099001, 'TOP@2': 0.6482742960944596, 'TOP@3': 0.726158038147139}}\n",
      "\t\t{'model_params': {'n_estimators': 460, 'max_depth': 5, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4550408719346049, 'TOP@2': 0.6791553133514986, 'TOP@3': 0.7522706630336058}}\n",
      "\t\t{'model_params': {'n_estimators': 970, 'max_depth': 7, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.4641235240690282, 'TOP@2': 0.697661217075386, 'TOP@3': 0.773047229791099}}\n",
      "\t\t{'model_params': {'n_estimators': 320, 'max_depth': 9, 'random_state': 1111}, 'valid_accuracy': {'TOP@1': 0.47014078110808355, 'TOP@2': 0.7064032697547684, 'TOP@3': 0.7886012715712988}}\n"
     ]
    }
   ],
   "source": [
    "for i, params in enumerate(data_sets):\n",
    "    params_str = {key: str(param) for key, param in params.items()}\n",
    "    print(f'\\nDataset variation {i+1}: {params_str}')\n",
    "    model_results = results[i]\n",
    "    for model_name, params in model_results.items():\n",
    "        print(f'\\t{model_name}:')\n",
    "        for j in range(len(params)):\n",
    "            print('\\t\\t{}'.format({\n",
    "                    'model_params': params[j]['model_params'],\n",
    "                    'valid_accuracy': params[j]['valid_accuracy']\n",
    "            }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic_regression test sets accuracies:\n",
      "\tDataset 1: {'TOP@1': 0.5401907356948229, 'TOP@2': 0.7184377838328792, 'TOP@3': 0.8038147138964578}\n",
      "\tDataset 2: {'TOP@1': 0.5331516802906449, 'TOP@2': 0.7158265213442325, 'TOP@3': 0.7993869209809265}\n",
      "\tDataset 3: {'TOP@1': 0.5322434150772025, 'TOP@2': 0.7124205267938238, 'TOP@3': 0.7958673932788374}\n",
      "\tDataset 4: {'TOP@1': 0.5403042688465032, 'TOP@2': 0.717983651226158, 'TOP@3': 0.8038147138964578}\n",
      "Best naive_bayes test sets accuracies:\n",
      "\tDataset 1: {'TOP@1': 0.553928247048138, 'TOP@2': 0.721503178928247, 'TOP@3': 0.8005222524977293}\n",
      "\tDataset 2: {'TOP@1': 0.5342870118074478, 'TOP@2': 0.6980018165304269, 'TOP@3': 0.7838328792007266}\n",
      "\tDataset 3: {'TOP@1': 0.5374659400544959, 'TOP@2': 0.7059491371480472, 'TOP@3': 0.7881471389645777}\n",
      "\tDataset 4: {'TOP@1': 0.5216848319709355, 'TOP@2': 0.6939146230699365, 'TOP@3': 0.7805404178019982}\n",
      "Best random_forest test sets accuracies:\n",
      "\tDataset 1: {'TOP@1': 0.4725249772933697, 'TOP@2': 0.6907356948228883, 'TOP@3': 0.7727066303360581}\n",
      "\tDataset 2: {'TOP@1': 0.4783151680290645, 'TOP@2': 0.7019754768392371, 'TOP@3': 0.7786103542234333}\n",
      "\tDataset 3: {'TOP@1': 0.476158038147139, 'TOP@2': 0.6974341507720254, 'TOP@3': 0.7762261580381471}\n",
      "\tDataset 4: {'TOP@1': 0.47888283378746593, 'TOP@2': 0.7096957311534968, 'TOP@3': 0.7867847411444142}\n"
     ]
    }
   ],
   "source": [
    "best_models = {\n",
    "    'logistic_regression': 0,\n",
    "    'naive_bayes': 3,\n",
    "    'random_forest': 3\n",
    "}\n",
    "\n",
    "for model_name, model_index in best_models.items():\n",
    "    print(f'Best {model_name} test sets accuracies:')\n",
    "    for i in range(len(results)):\n",
    "        print(f'\\tDataset {i+1}: {results[i][model_name][model_index][\"test_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp550 final project (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
