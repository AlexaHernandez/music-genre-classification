{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre classification using lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Helgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "E:\\Repos\\comp550-final-project\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.stem\n",
    "import nltk.corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.utils\n",
    "import sklearn.utils.testing\n",
    "import sklearn.exceptions\n",
    "\n",
    "random_state = 1111\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disperse</td>\n",
       "      <td>Tether</td>\n",
       "      <td>Warm-hearted\\nDirections\\nWe're off the map\\nI...</td>\n",
       "      <td>Alternative Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disperse</td>\n",
       "      <td>Foreword</td>\n",
       "      <td>[Instrumental]</td>\n",
       "      <td>Alternative Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disperse</td>\n",
       "      <td>Touching The Golden Cloud</td>\n",
       "      <td>Hear in this garden\\nHear in this space\\nImmer...</td>\n",
       "      <td>Alternative Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disperse</td>\n",
       "      <td>Neon</td>\n",
       "      <td>Hello dear stranger\\nI've got so much to tell ...</td>\n",
       "      <td>Alternative Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disperse</td>\n",
       "      <td>Kites</td>\n",
       "      <td>Still a headache\\nFrom last night\\nIt was vali...</td>\n",
       "      <td>Alternative Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>Flyleaf</td>\n",
       "      <td>Broken Wings</td>\n",
       "      <td>Thank you for being such a friend to me\\nOh, I...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>Flyleaf</td>\n",
       "      <td>Swept Away</td>\n",
       "      <td>The evil fell from your pretty mouth\\nWrapped ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>Flyleaf</td>\n",
       "      <td>Call You Out</td>\n",
       "      <td>How can you act like you know\\nWhen all you kn...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>Flyleaf</td>\n",
       "      <td>Beautiful Bride</td>\n",
       "      <td>Unified diversity\\nFunctioning as one body\\nEv...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>Flyleaf</td>\n",
       "      <td>Ocean Waves</td>\n",
       "      <td>So many wasted days\\nCome and go like ocean wa...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist                       song  \\\n",
       "0      Disperse                     Tether   \n",
       "1      Disperse                   Foreword   \n",
       "2      Disperse  Touching The Golden Cloud   \n",
       "3      Disperse                       Neon   \n",
       "4      Disperse                      Kites   \n",
       "...         ...                        ...   \n",
       "79995   Flyleaf               Broken Wings   \n",
       "79996   Flyleaf                 Swept Away   \n",
       "79997   Flyleaf               Call You Out   \n",
       "79998   Flyleaf            Beautiful Bride   \n",
       "79999   Flyleaf                Ocean Waves   \n",
       "\n",
       "                                                  lyrics             genre  \n",
       "0      Warm-hearted\\nDirections\\nWe're off the map\\nI...  Alternative Rock  \n",
       "1                                         [Instrumental]  Alternative Rock  \n",
       "2      Hear in this garden\\nHear in this space\\nImmer...  Alternative Rock  \n",
       "3      Hello dear stranger\\nI've got so much to tell ...  Alternative Rock  \n",
       "4      Still a headache\\nFrom last night\\nIt was vali...  Alternative Rock  \n",
       "...                                                  ...               ...  \n",
       "79995  Thank you for being such a friend to me\\nOh, I...              Rock  \n",
       "79996  The evil fell from your pretty mouth\\nWrapped ...              Rock  \n",
       "79997  How can you act like you know\\nWhen all you kn...              Rock  \n",
       "79998  Unified diversity\\nFunctioning as one body\\nEv...              Rock  \n",
       "79999  So many wasted days\\nCome and go like ocean wa...              Rock  \n",
       "\n",
       "[80000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('E:\\\\Repos\\\\comp550-final-project\\\\data\\\\scraped-lyrics-v1.csv')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Songs per genre available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Alternative Rock    8000\n",
       "Country             8000\n",
       "Hard Rock           8000\n",
       "Heavy Metal         8000\n",
       "Hip-Hop             8000\n",
       "Indie               8000\n",
       "Pop                 8000\n",
       "R&B                 8000\n",
       "Rock                8000\n",
       "Soul                8000\n",
       "Name: lyrics, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.groupby('genre').count().lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.tag_prefix_dict = {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        return [\n",
    "            self.normalizer.lemmatize(token, pos=self.get_tag_class(tag))\n",
    "            for token, tag in nltk.pos_tag(tokens)\n",
    "        ]\n",
    "    \n",
    "    def get_tag_class(self, tag):\n",
    "        prefix = tag[0].upper()\n",
    "        return self.tag_prefix_dict.get(prefix, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "class Stemmer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        return [\n",
    "            self.normalizer.stem(token)\n",
    "            for token in nltk.word_tokenize(document)\n",
    "        ]\n",
    "\n",
    "def fit_vectorizer(X_data, tokenizer, stop_words, min_df):    \n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop_words,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    vectorizer.fit_transform(X_data)\n",
    "    return vectorizer\n",
    "\n",
    "@sklearn.utils.testing.ignore_warnings(category=sklearn.exceptions.ConvergenceWarning)\n",
    "def random_search(models, search_params, n_datasets=1, n_models=1):\n",
    "    data_sets = []\n",
    "    results = [{} for i in range(n_datasets)]\n",
    "\n",
    "    data_set_variations = [\n",
    "        choose_random_params(search_params['data'])\n",
    "        for i in range(n_datasets)\n",
    "    ]\n",
    "    model_variations = {\n",
    "        model_name: [choose_random_params(search_params['model'][model_name]) for i in range(n_models)]\n",
    "        for model_name in search_params['model'].keys()\n",
    "    }\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        print(f'Data set variation {i+1}/{n_datasets}')\n",
    "\n",
    "        data_params = data_set_variations[i]\n",
    "        data_sets.append(data_params)\n",
    "\n",
    "        print('\\tFitting vectorizer...')\n",
    "        vectorizer = fit_vectorizer(X_train_raw, **data_params)\n",
    "\n",
    "        X_train = vectorizer.transform(X_train_raw)\n",
    "        X_valid = vectorizer.transform(X_valid_raw)\n",
    "        X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "        for model_name, model_class in models.items():\n",
    "            for j in range(n_models):\n",
    "                print(f'\\t{model_name} {j+1}/{n_models}')\n",
    "\n",
    "                model_params = model_variations[model_name][j]\n",
    "\n",
    "                model = model_class(**model_params)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                valid_predictions = model.predict(X_valid)\n",
    "                test_predictions = model.predict(X_test)\n",
    "                \n",
    "                valid_accuracy = sklearn.metrics.accuracy_score(y_valid, valid_predictions)\n",
    "                \n",
    "                # This number is only looked at once at the very end when the best models have been chosen based on validation accuracy\n",
    "                test_accuracy = sklearn.metrics.accuracy_score(y_test, test_predictions)\n",
    "                test_confusion_matrix = sklearn.metrics.confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "                if results[i].get(model_name, None) is None:\n",
    "                    results[i][model_name] = []\n",
    "\n",
    "                results[i][model_name].append({\n",
    "                    'model_params': model_params,\n",
    "                    'valid_accuracy': valid_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'test_confusion_matrix': test_confusion_matrix\n",
    "                })\n",
    "    \n",
    "    return data_sets, results\n",
    "\n",
    "def choose_random_params(parameters):\n",
    "    return {\n",
    "        name: np.random.choice(values)\n",
    "        for name, values in parameters.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre_ratio: 1\n",
      "song_ratio: 1\n",
      "Genres used: ['Alternative Rock' 'Country' 'Hard Rock' 'Soul' 'Heavy Metal' 'Hip-Hop'\n",
      " 'Indie' 'Pop' 'R&B' 'Rock']\n",
      "Ratio of data used: 80000/80000 = 1.00\n"
     ]
    }
   ],
   "source": [
    "splits = [0.7, 0.85]\n",
    "\n",
    "genres = dataframe.genre.unique()\n",
    "genre_ratio = 1 # How many genres to use\n",
    "song_ratio = 1 # How many songs per genre to use\n",
    "\n",
    "all_data = []\n",
    "for genre in genres[ :int(genre_ratio*len(genres))]:\n",
    "    lyrics = dataframe[dataframe.genre == genre].lyrics\n",
    "    all_data += [(lyric, genre) for lyric in lyrics[ : int(song_ratio*len(lyrics))]]\n",
    "\n",
    "all_data = sklearn.utils.shuffle(np.array(all_data), random_state=random_state)\n",
    "\n",
    "n = all_data.shape[0]\n",
    "\n",
    "X_train_raw, y_train = all_data[:int(splits[0]*n), 0], all_data[:int(splits[0]*n), 1]\n",
    "X_valid_raw, y_valid = all_data[int(splits[0]*n):int(splits[1]*n), 0], all_data[int(splits[0]*n): int(splits[1]*n), 1]\n",
    "X_test_raw, y_test = all_data[int(splits[1]*n):, 0], all_data[int(splits[1]*n):, 1]\n",
    "\n",
    "print(f'genre_ratio: {genre_ratio}')\n",
    "print(f'song_ratio: {song_ratio}')\n",
    "print(f'Genres used: {genres[ :int(genre_ratio*len(genres))]}')\n",
    "print(f'Ratio of data used: {n}/{len(dataframe)} = {(n/len(dataframe)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set hyper-parameter search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set variation 1/2\n",
      "\tFitting vectorizer...\n",
      "\tlogistic_regression 1/3\n",
      "\tlogistic_regression 2/3\n",
      "\tlogistic_regression 3/3\n",
      "\tnaive_bayes 1/3\n",
      "\tnaive_bayes 2/3\n",
      "\tnaive_bayes 3/3\n",
      "Data set variation 2/2\n",
      "\tFitting vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Repos\\comp550-final-project\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlogistic_regression 1/3\n",
      "\tlogistic_regression 2/3\n",
      "\tlogistic_regression 3/3\n",
      "\tnaive_bayes 1/3\n",
      "\tnaive_bayes 2/3\n",
      "\tnaive_bayes 3/3\n",
      "\n",
      "Dataset variation 1: {'tokenizer': <__main__.Lemmatizer object at 0x000001F9D22B9308>, 'stop_words': None, 'min_df': 2}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.27791666666666665}\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.30391666666666667}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.27791666666666665}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.4}, 'valid_accuracy': 0.3631666666666667}\n",
      "\t\t{'model_params': {'alpha': 0.5}, 'valid_accuracy': 0.36283333333333334}\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': 0.3605}\n",
      "\n",
      "Dataset variation 2: {'tokenizer': <__main__.Stemmer object at 0x000001F9D193B488>, 'stop_words': 'english', 'min_df': 3}\n",
      "\tlogistic_regression:\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.2911666666666667}\n",
      "\t\t{'model_params': {'eta0': 0.001, 'alpha': 0.1, 'max_iter': 1, 'random_state': 1111}, 'valid_accuracy': 0.31416666666666665}\n",
      "\t\t{'model_params': {'eta0': 0.1, 'alpha': 0.001, 'max_iter': 3, 'random_state': 1111}, 'valid_accuracy': 0.2911666666666667}\n",
      "\tnaive_bayes:\n",
      "\t\t{'model_params': {'alpha': 0.4}, 'valid_accuracy': 0.36825}\n",
      "\t\t{'model_params': {'alpha': 0.5}, 'valid_accuracy': 0.36716666666666664}\n",
      "\t\t{'model_params': {'alpha': 0.9}, 'valid_accuracy': 0.3650833333333333}\n"
     ]
    }
   ],
   "source": [
    "search_params = {\n",
    "    'data': {\n",
    "        'tokenizer': [Lemmatizer(), Stemmer()],\n",
    "        'stop_words': ['english', None],\n",
    "        'min_df': [1, 2, 3] # Minimum token frequency\n",
    "    },\n",
    "    'model': {\n",
    "        'logistic_regression': {\n",
    "            'eta0': [1e-3, 1e-2, 1e-1], # learning rate\n",
    "            'alpha': [1e-3, 1e-2, 1e-1], # regularization\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'linear_support_vector_machine': {\n",
    "            'kernel': ['linear'],\n",
    "            'max_iter': np.arange(start=1, stop=5), # epochs\n",
    "            'C': [1e-3, 1e-2, 1e-1], # L2 regularization\n",
    "            'random_state': [random_state]\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'alpha': np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'n_estimators': np.arange(start=10, stop=1000, step=10),\n",
    "            'max_depth': np.append(np.array(None), np.arange(start=1, stop=5, step=2)),\n",
    "            'random_state': [random_state]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': sklearn.linear_model.SGDClassifier,\n",
    "    #'linear_support_vector_machine': sklearn.svm.SVC,\n",
    "    'naive_bayes': sklearn.naive_bayes.MultinomialNB,\n",
    "    #'random_forest': sklearn.ensemble.RandomForestClassifier\n",
    "}\n",
    "\n",
    "data_sets, results = random_search(models, search_params, n_datasets=2, n_models=3)\n",
    "\n",
    "for i, params in enumerate(data_sets):\n",
    "    print(f'\\nDataset variation {i+1}: {params}')\n",
    "    model_results = results[i]\n",
    "    for model_name, params in model_results.items():\n",
    "        print(f'\\t{model_name}:')\n",
    "        for j in range(len(params)):\n",
    "            print('\\t\\t{}'.format({\n",
    "                    'model_params': params[j]['model_params'],\n",
    "                    'valid_accuracy': params[j]['valid_accuracy']\n",
    "            }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic_regression test sets accuracies:\n",
      "\tDataset 1: 0.305\n",
      "\tDataset 2: 0.317\n",
      "Best naive_bayes test sets accuracies:\n",
      "\tDataset 1: 0.363\n",
      "\tDataset 2: 0.367\n",
      "Random classifier test set accuracy: 0.099\n",
      "\n",
      "Genres: ['Alternative Rock' 'Country' 'Hard Rock' 'Soul' 'Heavy Metal' 'Hip-Hop'\n",
      " 'Indie' 'Pop' 'R&B' 'Rock']\n",
      "\n",
      "Best model (Naive Bayes) confusion matrix:\n",
      "[[448  67  47  99  29 265  93  21  61  63]\n",
      " [ 90 731  22  47  12  75  67  13  18 102]\n",
      " [206  84 186 287  24  81  78  29  64 145]\n",
      " [129  28  81 760  17  32  24   9  43  48]\n",
      " [ 39   9   7  11 719  28 156 181   3  38]\n",
      " [344 103  32 113  21 370 117  18  62  84]\n",
      " [231  60  27  51  87 149 359 123  24 123]\n",
      " [ 85  76  17  34 188  61 203 213  16 291]\n",
      " [322 128  78 180  18 189  78  26  82 109]\n",
      " [111 149  26  45  51  94  92 111  25 488]]\n"
     ]
    }
   ],
   "source": [
    "best_models = {\n",
    "    'logistic_regression': 1,\n",
    "    'naive_bayes': 0\n",
    "}\n",
    "\n",
    "for model_name, model_index in best_models.items():\n",
    "    print(f'Best {model_name} test sets accuracies:')\n",
    "    for i in range(len(results)):\n",
    "        print(f'\\tDataset {i+1}: {results[i][model_name][model_index][\"test_accuracy\"]:.3f}')\n",
    "\n",
    "random_predictions = np.random.choice(genres, size=y_test.shape[0])\n",
    "random_accuracy = sklearn.metrics.accuracy_score(y_test, random_predictions)\n",
    "print(f'Random classifier test set accuracy: {random_accuracy:.3f}', end='\\n\\n')\n",
    "\n",
    "print(f'Genres: {genres}', end='\\n\\n')\n",
    "print('Best model (Naive Bayes) confusion matrix:')\n",
    "print(results[0]['naive_bayes'][0]['test_confusion_matrix'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp550 final project (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
